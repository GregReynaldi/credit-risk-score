{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced Explainability Analysis for Credit Risk Assessment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import shap\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "ROOT = os.path.abspath(os.getcwd())\n",
        "PROJECT_ROOT = os.path.abspath(os.path.join(ROOT, '..'))\n",
        "\n",
        "MODELS_DIR = os.path.join(PROJECT_ROOT, 'models')\n",
        "DATASET_DIR = os.path.join(PROJECT_ROOT, 'dataset')\n",
        "ARTIFACTS_DIR = os.path.join(PROJECT_ROOT, 'artifacts')\n",
        "OUTPUT_DIR = os.path.join(ARTIFACTS_DIR, '04b_images')\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data and Model Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data shape: (26064, 17)\n",
            "Test data shape: (6517, 17)\n",
            "Number of features: 17\n"
          ]
        }
      ],
      "source": [
        "X_train = pd.read_pickle(os.path.join(DATASET_DIR, 'X_train.pkl'))\n",
        "X_test = pd.read_pickle(os.path.join(DATASET_DIR, 'X_test.pkl'))\n",
        "y_test = pd.read_pickle(os.path.join(DATASET_DIR, 'y_test.pkl'))\n",
        "\n",
        "if isinstance(y_test, pd.DataFrame):\n",
        "    y_test = y_test.iloc[:, 0]\n",
        "\n",
        "feature_names = X_train.columns.tolist()\n",
        "\n",
        "print(f\"Training data shape: {X_train.shape}\")\n",
        "print(f\"Test data shape: {X_test.shape}\")\n",
        "print(f\"Number of features: {len(feature_names)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ensemble components loaded successfully\n"
          ]
        }
      ],
      "source": [
        "def _require_model_file(filename):\n",
        "    path = os.path.join(MODELS_DIR, filename)\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"Missing '{filename}' in {MODELS_DIR}. Did you run the training notebook?\")\n",
        "    return path\n",
        "\n",
        "base_models_dict = joblib.load(_require_model_file('ensemble_base_models_neural.joblib'))\n",
        "meta_model = keras.models.load_model(_require_model_file('meta_learner_neural.h5'), compile=False)\n",
        "neural_network_model = keras.models.load_model(_require_model_file('residual_neural.h5'), compile=False)\n",
        "\n",
        "xgb_deep = base_models_dict['xgb_deep']\n",
        "xgb_shallow = base_models_dict['xgb_shallow']\n",
        "lgbm_fast = base_models_dict['lgbm_fast']\n",
        "catboost_robust = base_models_dict['catboost_robust']\n",
        "\n",
        "print(\"Ensemble components loaded successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ensemble_predict(): Combines base model predictions via meta-learner stacking\n",
        "# Parameters: X (DataFrame or array) - input features\n",
        "# Returns: array - probability predictions from stacked ensemble\n",
        "def ensemble_predict(X):\n",
        "    if isinstance(X, pd.DataFrame):\n",
        "        X = X.values\n",
        "    \n",
        "    pred_xgb_deep = xgb_deep.predict_proba(X)[:, 1]\n",
        "    pred_xgb_shallow = xgb_shallow.predict_proba(X)[:, 1]\n",
        "    pred_lgbm = lgbm_fast.predict_proba(X)[:, 1]\n",
        "    pred_catboost = catboost_robust.predict_proba(X)[:, 1]\n",
        "    pred_neural = neural_network_model.predict(X, verbose=0).ravel()\n",
        "    \n",
        "    base_preds = np.column_stack([pred_xgb_deep, pred_xgb_shallow, pred_lgbm, pred_catboost, pred_neural])\n",
        "    final_pred = meta_model.predict(base_preds, verbose=0).ravel()\n",
        "    \n",
        "    return final_pred\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model-Specific SHAP Comparison\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Background Data Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Background data size: 200\n",
            "Test sample size: 100\n"
          ]
        }
      ],
      "source": [
        "background_data = X_train.sample(n=min(200, len(X_train)), random_state=42).values\n",
        "test_sample_size = min(100, len(X_test))\n",
        "X_test_sample = X_test.iloc[:test_sample_size].values\n",
        "\n",
        "print(f\"Background data size: {background_data.shape[0]}\")\n",
        "print(f\"Test sample size: {test_sample_size}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Individual Model SHAP Computation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing SHAP values for individual models...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PermutationExplainer explainer: 101it [00:11,  2.95it/s]                         \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  XGBoost Deep: Done\n",
            "  XGBoost Shallow: Done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception in thread Thread-5 (_readerthread):\n",
            "Traceback (most recent call last):\n",
            "  File \"d:\\Conda\\envs\\final_last\\lib\\threading.py\", line 1016, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"d:\\Conda\\envs\\final_last\\lib\\threading.py\", line 953, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"d:\\Conda\\envs\\final_last\\lib\\subprocess.py\", line 1515, in _readerthread\n",
            "    buffer.append(fh.read())\n",
            "  File \"d:\\Conda\\envs\\final_last\\lib\\codecs.py\", line 322, in decode\n",
            "    (result, consumed) = self._buffer_decode(data, self.errors, final)\n",
            "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xce in position 4: invalid continuation byte\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  LightGBM: Done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PermutationExplainer explainer: 101it [00:19,  2.57it/s]                         \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  CatBoost: Done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PermutationExplainer explainer: 101it [06:40,  4.09s/it]                         "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Neural Network: Done\n",
            "All SHAP values computed\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Compute SHAP values separately for each base model\n",
        "# This lets us see which models rely on which features differently\n",
        "# PermutationExplainer works by shuffling features and measuring impact\n",
        "model_explainers = {}\n",
        "model_shap_values = {}\n",
        "\n",
        "print(\"Computing SHAP values for individual models...\")\n",
        "\n",
        "# XGBoost Deep\n",
        "explainer_xgb_deep = shap.PermutationExplainer(\n",
        "    lambda X: xgb_deep.predict_proba(X)[:, 1],\n",
        "    background_data\n",
        ")\n",
        "shap_xgb_deep = explainer_xgb_deep(X_test_sample)\n",
        "model_shap_values['xgb_deep'] = shap_xgb_deep.values\n",
        "print(\"  XGBoost Deep: Done\")\n",
        "\n",
        "# XGBoost Shallow\n",
        "explainer_xgb_shallow = shap.PermutationExplainer(\n",
        "    lambda X: xgb_shallow.predict_proba(X)[:, 1],\n",
        "    background_data\n",
        ")\n",
        "shap_xgb_shallow = explainer_xgb_shallow(X_test_sample)\n",
        "model_shap_values['xgb_shallow'] = shap_xgb_shallow.values\n",
        "print(\"  XGBoost Shallow: Done\")\n",
        "\n",
        "# LightGBM\n",
        "explainer_lgbm = shap.PermutationExplainer(\n",
        "    lambda X: lgbm_fast.predict_proba(X)[:, 1],\n",
        "    background_data\n",
        ")\n",
        "shap_lgbm = explainer_lgbm(X_test_sample)\n",
        "model_shap_values['lgbm'] = shap_lgbm.values\n",
        "print(\"  LightGBM: Done\")\n",
        "\n",
        "# CatBoost\n",
        "explainer_catboost = shap.PermutationExplainer(\n",
        "    lambda X: catboost_robust.predict_proba(X)[:, 1],\n",
        "    background_data\n",
        ")\n",
        "shap_catboost = explainer_catboost(X_test_sample)\n",
        "model_shap_values['catboost'] = shap_catboost.values\n",
        "print(\"  CatBoost: Done\")\n",
        "\n",
        "# Neural Network\n",
        "explainer_nn = shap.PermutationExplainer(\n",
        "    lambda X: neural_network_model.predict(X, verbose=0).ravel(),\n",
        "    background_data\n",
        ")\n",
        "shap_nn = explainer_nn(X_test_sample)\n",
        "model_shap_values['neural_network'] = shap_nn.values\n",
        "print(\"  Neural Network: Done\")\n",
        "\n",
        "print(\"All SHAP values computed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Importance Comparison Across Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 10 features by average importance across all models:\n",
            "                             xgb_deep  xgb_shallow      lgbm  catboost  \\\n",
            "loan_grade                   0.086685     0.104630  0.095004  0.099147   \n",
            "loan_percent_income          0.078916     0.084808  0.086118  0.088837   \n",
            "person_income                0.087958     0.073468  0.081706  0.063153   \n",
            "loan_intent_VENTURE          0.051779     0.050734  0.053481  0.047652   \n",
            "person_home_ownership_RENT   0.045805     0.039029  0.041822  0.040184   \n",
            "person_home_ownership_OWN    0.022990     0.027919  0.025549  0.024274   \n",
            "loan_int_rate                0.025680     0.022979  0.025413  0.024311   \n",
            "loan_intent_HOMEIMPROVEMENT  0.020085     0.014059  0.018264  0.024699   \n",
            "loan_amnt                    0.025162     0.008627  0.011816  0.007262   \n",
            "loan_intent_EDUCATION        0.016257     0.016403  0.014982  0.015001   \n",
            "\n",
            "                             neural_network  \n",
            "loan_grade                         0.078515  \n",
            "loan_percent_income                0.083204  \n",
            "person_income                      0.037156  \n",
            "loan_intent_VENTURE                0.043313  \n",
            "person_home_ownership_RENT         0.037155  \n",
            "person_home_ownership_OWN          0.014755  \n",
            "loan_int_rate                      0.013576  \n",
            "loan_intent_HOMEIMPROVEMENT        0.014368  \n",
            "loan_amnt                          0.032916  \n",
            "loan_intent_EDUCATION              0.016748  \n"
          ]
        }
      ],
      "source": [
        "# Calculate average feature importance for each model\n",
        "# Then compare across models to see which features are consistently important\n",
        "model_importances = {}\n",
        "\n",
        "for model_name, shap_vals in model_shap_values.items():\n",
        "    # Average absolute SHAP values = feature importance for this model\n",
        "    importance = np.mean(np.abs(shap_vals), axis=0)\n",
        "    model_importances[model_name] = importance\n",
        "\n",
        "importance_df = pd.DataFrame(model_importances, index=feature_names)\n",
        "# Sort by average importance across all models\n",
        "feature_mean_importance = importance_df.mean(axis=1)\n",
        "importance_df = importance_df.loc[feature_mean_importance.sort_values(ascending=False).index]\n",
        "\n",
        "print(\"Top 10 features by average importance across all models:\")\n",
        "print(importance_df.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Agreement Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "top_n_features = 12\n",
        "top_features = importance_df.head(top_n_features).index.tolist()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "x = np.arange(len(top_features))\n",
        "width = 0.15\n",
        "\n",
        "models_list = ['xgb_deep', 'xgb_shallow', 'lgbm', 'catboost', 'neural_network']\n",
        "colors = ['#E63946', '#F77F00', '#FCBF49', '#06A77D', '#2E86AB']\n",
        "\n",
        "for idx, model_name in enumerate(models_list):\n",
        "    values = [importance_df.loc[feat, model_name] for feat in top_features]\n",
        "    ax.bar(x + idx * width, values, width, label=model_name.replace('_', ' ').title(), \n",
        "           alpha=0.85, color=colors[idx])\n",
        "\n",
        "ax.set_xlabel('Features', fontsize=13, fontweight='bold')\n",
        "ax.set_ylabel('Mean |SHAP Value|', fontsize=13, fontweight='bold')\n",
        "ax.set_title('Feature Importance Comparison Across Base Models', fontsize=15, fontweight='bold')\n",
        "ax.set_xticks(x + width * 2)\n",
        "ax.set_xticklabels(top_features, rotation=45, ha='right', fontsize=10)\n",
        "ax.legend(fontsize=11, loc='upper right')\n",
        "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, 'model_shap_comparison.png'), dpi=300, bbox_inches='tight')\n",
        "plt.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Disagreement Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 10 features with highest model disagreement (CV):\n",
            "                        feature  mean_importance  coefficient_of_variation\n",
            "15          loan_intent_VENTURE         0.001826                  0.832700\n",
            "16    cb_person_default_on_file         0.000275                  0.694993\n",
            "8   person_home_ownership_OTHER         0.017157                  0.659193\n",
            "14         loan_intent_PERSONAL         0.003013                  0.556767\n",
            "11        loan_intent_EDUCATION         0.011671                  0.318773\n",
            "12  loan_intent_HOMEIMPROVEMENT         0.011455                  0.309435\n",
            "2             person_emp_length         0.068688                  0.290131\n",
            "7                    loan_grade         0.018295                  0.240720\n",
            "6    cb_person_cred_hist_length         0.022392                  0.225160\n",
            "5           loan_percent_income         0.023097                  0.216745\n"
          ]
        }
      ],
      "source": [
        "# Measure model disagreement: which features do models disagree about?\n",
        "# High coefficient of variation = models have very different opinions on this feature\n",
        "# Low CV = models agree this feature is important (or unimportant)\n",
        "importance_std = importance_df.std(axis=1)\n",
        "importance_mean = importance_df.mean(axis=1)\n",
        "\n",
        "# Coefficient of variation = std / mean (normalized disagreement measure)\n",
        "disagreement_df = pd.DataFrame({\n",
        "    'feature': feature_names,\n",
        "    'mean_importance': importance_mean.values,\n",
        "    'std_importance': importance_std.values,\n",
        "    'coefficient_of_variation': (importance_std / (importance_mean + 1e-9)).values\n",
        "}).sort_values('coefficient_of_variation', ascending=False)\n",
        "\n",
        "print(\"Top 10 features with highest model disagreement (CV):\")\n",
        "print(disagreement_df.head(10)[['feature', 'mean_importance', 'coefficient_of_variation']])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Disagreement Heatmap\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "top_disagree_features = disagreement_df.head(10)['feature'].tolist()\n",
        "disagreement_subset = importance_df.loc[top_disagree_features]\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(disagreement_subset, annot=True, fmt='.4f', cmap='YlOrRd', \n",
        "            xticklabels=[m.replace('_', ' ').title() for m in models_list],\n",
        "            yticklabels=top_disagree_features, cbar_kws={'label': 'Feature Importance'})\n",
        "plt.title('Model Disagreement: Top 10 Features with Highest Variance', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, 'model_disagreement_heatmap.png'), dpi=300, bbox_inches='tight')\n",
        "plt.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sensitivity Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Perturbation Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sensitivity analysis sample size: 50\n"
          ]
        }
      ],
      "source": [
        "sensitivity_sample_size = min(50, len(X_test))\n",
        "X_sensitivity = X_test.iloc[:sensitivity_sample_size].values\n",
        "original_predictions = ensemble_predict(X_sensitivity)\n",
        "\n",
        "print(f\"Sensitivity analysis sample size: {sensitivity_sample_size}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sensitivity Computation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 10 most sensitive features:\n",
            "                        feature  sensitivity\n",
            "1                 person_income     0.035567\n",
            "7                    loan_grade     0.025388\n",
            "5           loan_percent_income     0.019726\n",
            "10   person_home_ownership_RENT     0.009533\n",
            "2             person_emp_length     0.007578\n",
            "4                 loan_int_rate     0.007157\n",
            "3                     loan_amnt     0.007076\n",
            "15          loan_intent_VENTURE     0.003998\n",
            "12  loan_intent_HOMEIMPROVEMENT     0.003363\n",
            "11        loan_intent_EDUCATION     0.002718\n"
          ]
        }
      ],
      "source": [
        "# Sensitivity analysis: test how much predictions change when we tweak each feature\n",
        "# More sensitive features = small changes cause big prediction changes\n",
        "# Less sensitive = predictions stay stable when feature changes\n",
        "perturbation_levels = [0.1, 0.2]  # Try ±10% and ±20% changes\n",
        "feature_sensitivity = np.zeros(len(feature_names))\n",
        "\n",
        "for feat_idx in range(len(feature_names)):\n",
        "    sensitivity_scores = []\n",
        "    \n",
        "    for pert_level in perturbation_levels:\n",
        "        X_perturbed_plus = X_sensitivity.copy()\n",
        "        X_perturbed_minus = X_sensitivity.copy()\n",
        "        \n",
        "        # Increase and decrease feature by same percentage\n",
        "        X_perturbed_plus[:, feat_idx] *= (1 + pert_level)\n",
        "        X_perturbed_minus[:, feat_idx] *= (1 - pert_level)\n",
        "        \n",
        "        # Measure prediction change\n",
        "        pred_plus = ensemble_predict(X_perturbed_plus)\n",
        "        pred_minus = ensemble_predict(X_perturbed_minus)\n",
        "        \n",
        "        diff_plus = np.mean(np.abs(original_predictions - pred_plus))\n",
        "        diff_minus = np.mean(np.abs(original_predictions - pred_minus))\n",
        "        \n",
        "        # Average the two directions\n",
        "        sensitivity_scores.append((diff_plus + diff_minus) / 2)\n",
        "    \n",
        "    # Average across perturbation levels\n",
        "    feature_sensitivity[feat_idx] = np.mean(sensitivity_scores)\n",
        "\n",
        "sensitivity_df = pd.DataFrame({\n",
        "    'feature': feature_names,\n",
        "    'sensitivity': feature_sensitivity\n",
        "}).sort_values('sensitivity', ascending=False)\n",
        "\n",
        "print(\"Top 10 most sensitive features:\")\n",
        "print(sensitivity_df.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sensitivity Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "top_sensitive_features = sensitivity_df.head(10)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 7))\n",
        "ax.barh(range(len(top_sensitive_features)), top_sensitive_features['sensitivity'], \n",
        "        color='#E63946', alpha=0.8)\n",
        "ax.set_yticks(range(len(top_sensitive_features)))\n",
        "ax.set_yticklabels(top_sensitive_features['feature'], fontsize=11)\n",
        "ax.set_xlabel('Sensitivity Score (Mean Prediction Change)', fontsize=13, fontweight='bold')\n",
        "ax.set_title('Top 10 Most Sensitive Features', fontsize=15, fontweight='bold')\n",
        "ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
        "ax.invert_yaxis()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, 'feature_sensitivity_ranking.png'), dpi=300, bbox_inches='tight')\n",
        "plt.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sensitivity Curves for Top Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create sensitivity curves: show how predictions change as feature varies\n",
        "# X-axis = feature change percentage, Y-axis = average prediction\n",
        "top_3_sensitive = sensitivity_df.head(3)['feature'].tolist()\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "for idx, feat_name in enumerate(top_3_sensitive):\n",
        "    feat_idx = feature_names.index(feat_name)\n",
        "    \n",
        "    # Vary feature from -30% to +30% around median value\n",
        "    base_value = np.median(X_sensitivity[:, feat_idx])\n",
        "    pert_range = np.linspace(-0.3, 0.3, 20)\n",
        "    \n",
        "    sensitivity_curve = []\n",
        "    for pert in pert_range:\n",
        "        # Set all samples to same feature value (base * (1 + pert))\n",
        "        X_pert = X_sensitivity.copy()\n",
        "        X_pert[:, feat_idx] = base_value * (1 + pert)\n",
        "        pred_pert = ensemble_predict(X_pert)\n",
        "        sensitivity_curve.append(np.mean(pred_pert))\n",
        "    \n",
        "    axes[idx].plot(pert_range * 100, sensitivity_curve, linewidth=2.5, color='#2E86AB', marker='o', markersize=4)\n",
        "    axes[idx].axhline(y=np.mean(original_predictions), color='r', linestyle='--', alpha=0.7, label='Original Mean')\n",
        "    axes[idx].set_xlabel(f'{feat_name} Perturbation (%)', fontsize=11, fontweight='bold')\n",
        "    axes[idx].set_ylabel('Mean Prediction', fontsize=11)\n",
        "    axes[idx].set_title(f'Sensitivity Curve: {feat_name}', fontsize=12, fontweight='bold')\n",
        "    axes[idx].grid(True, alpha=0.3)\n",
        "    axes[idx].legend(fontsize=9)\n",
        "\n",
        "plt.suptitle('Sensitivity Curves for Top 3 Most Sensitive Features', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, 'sensitivity_curves.png'), dpi=300, bbox_inches='tight')\n",
        "plt.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Decision Boundary Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dimensionality Reduction Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decision boundary analysis sample size: 500\n"
          ]
        }
      ],
      "source": [
        "db_sample_size = min(500, len(X_test))\n",
        "X_db_sample = X_test.iloc[:db_sample_size].values\n",
        "y_db_sample = y_test.iloc[:db_sample_size].values if hasattr(y_test, 'iloc') else y_test[:db_sample_size]\n",
        "predictions_db = ensemble_predict(X_db_sample)\n",
        "\n",
        "print(f\"Decision boundary analysis sample size: {db_sample_size}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PCA Projection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PCA explained variance ratio: [0.14061448 0.1327594 ]\n",
            "Total variance explained: 0.273\n"
          ]
        }
      ],
      "source": [
        "# PCA: reduces 17 features to 2 dimensions for visualization\n",
        "# Finds the 2 directions that capture most variance in the data\n",
        "# This lets us see decision boundaries in 2D plots\n",
        "scaler = StandardScaler()\n",
        "X_db_scaled = scaler.fit_transform(X_db_sample)\n",
        "\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "X_pca = pca.fit_transform(X_db_scaled)\n",
        "\n",
        "print(f\"PCA explained variance ratio: {pca.explained_variance_ratio_}\")\n",
        "print(f\"Total variance explained: {pca.explained_variance_ratio_.sum():.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Decision Boundary Visualization (PCA)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Plot 1: Color by actual label\n",
        "scatter1 = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=y_db_sample, \n",
        "                           cmap='RdYlGn', alpha=0.6, s=30, edgecolors='black', linewidth=0.5)\n",
        "axes[0].set_xlabel('First Principal Component', fontsize=12, fontweight='bold')\n",
        "axes[0].set_ylabel('Second Principal Component', fontsize=12, fontweight='bold')\n",
        "axes[0].set_title('PCA Projection: Actual Labels', fontsize=13, fontweight='bold')\n",
        "plt.colorbar(scatter1, ax=axes[0], label='Actual Risk (0=Low, 1=High)')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Color by prediction probability\n",
        "scatter2 = axes[1].scatter(X_pca[:, 0], X_pca[:, 1], c=predictions_db, \n",
        "                          cmap='RdYlGn', alpha=0.6, s=30, edgecolors='black', linewidth=0.5)\n",
        "axes[1].set_xlabel('First Principal Component', fontsize=12, fontweight='bold')\n",
        "axes[1].set_ylabel('Second Principal Component', fontsize=12, fontweight='bold')\n",
        "axes[1].set_title('PCA Projection: Predicted Probabilities', fontsize=13, fontweight='bold')\n",
        "plt.colorbar(scatter2, ax=axes[1], label='Predicted Risk Probability')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('Decision Boundary Analysis using PCA', fontsize=15, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, 'decision_boundary_pca.png'), dpi=300, bbox_inches='tight')\n",
        "plt.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Risk Threshold Regions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Group predictions into risk categories for visualization\n",
        "# Low risk: <30%, Medium: 30-70%, High: ≥70%\n",
        "risk_categories = np.zeros(len(predictions_db))\n",
        "risk_categories[predictions_db < 0.3] = 0  # Low risk\n",
        "risk_categories[(predictions_db >= 0.3) & (predictions_db < 0.7)] = 1  # Medium risk\n",
        "risk_categories[predictions_db >= 0.7] = 2  # High risk\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], c=risk_categories, \n",
        "                    cmap='RdYlGn', alpha=0.7, s=40, edgecolors='black', linewidth=0.5)\n",
        "ax.set_xlabel('First Principal Component', fontsize=13, fontweight='bold')\n",
        "ax.set_ylabel('Second Principal Component', fontsize=13, fontweight='bold')\n",
        "ax.set_title('Risk Threshold Regions (Low/Medium/High)', fontsize=14, fontweight='bold')\n",
        "cbar = plt.colorbar(scatter, ax=ax, ticks=[0, 1, 2])\n",
        "cbar.set_ticklabels(['Low Risk (<0.3)', 'Medium Risk (0.3-0.7)', 'High Risk (≥0.7)'])\n",
        "cbar.set_label('Risk Category', fontsize=11)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, 'risk_threshold_regions.png'), dpi=300, bbox_inches='tight')\n",
        "plt.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Explanation Consistency Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Instance Clustering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Clustered 200 instances into 5 clusters\n",
            "Cluster sizes: [26 36 39 32 67]\n"
          ]
        }
      ],
      "source": [
        "consistency_sample_size = min(200, len(X_test))\n",
        "X_consistency = X_test.iloc[:consistency_sample_size].values\n",
        "\n",
        "# Cluster similar instances together to test explanation consistency\n",
        "# If instances are similar, their SHAP explanations should also be similar\n",
        "scaler_consistency = StandardScaler()\n",
        "X_consistency_scaled = scaler_consistency.fit_transform(X_consistency)\n",
        "\n",
        "# Group into 5 clusters based on feature similarity\n",
        "n_clusters = 5\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "cluster_labels = kmeans.fit_predict(X_consistency_scaled)\n",
        "\n",
        "print(f\"Clustered {consistency_sample_size} instances into {n_clusters} clusters\")\n",
        "print(f\"Cluster sizes: {np.bincount(cluster_labels)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SHAP Values for Consistency Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PermutationExplainer explainer: 201it [19:24,  5.85s/it]                         "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SHAP values computed for 200 instances\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Compute SHAP values for clustered instances\n",
        "# We'll check if instances in same cluster have similar SHAP values (consistent explanations)\n",
        "explainer_ensemble = shap.PermutationExplainer(ensemble_predict, background_data)\n",
        "shap_consistency = explainer_ensemble(X_consistency)\n",
        "shap_consistency_values = shap_consistency.values\n",
        "\n",
        "print(f\"SHAP values computed for {consistency_sample_size} instances\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Consistency Metrics Computation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cluster consistency metrics:\n",
            "  Cluster 0: size=26, mean_CV=0.8686\n",
            "  Cluster 1: size=36, mean_CV=1.2416\n",
            "  Cluster 2: size=39, mean_CV=1.5320\n",
            "  Cluster 3: size=32, mean_CV=1.0487\n",
            "  Cluster 4: size=67, mean_CV=1.0565\n"
          ]
        }
      ],
      "source": [
        "# Check explanation consistency: do similar instances get similar explanations?\n",
        "# For each cluster, measure how much SHAP values vary within the cluster\n",
        "# Low variation = consistent explanations (good), high variation = inconsistent (bad)\n",
        "cluster_consistency = {}\n",
        "\n",
        "for cluster_id in range(n_clusters):\n",
        "    cluster_mask = cluster_labels == cluster_id\n",
        "    cluster_shap = shap_consistency_values[cluster_mask]\n",
        "    \n",
        "    if len(cluster_shap) > 1:\n",
        "        # Average SHAP importance for features in this cluster\n",
        "        mean_shap = np.mean(np.abs(cluster_shap), axis=0)\n",
        "        # How much do SHAP values vary within this cluster?\n",
        "        std_shap = np.std(np.abs(cluster_shap), axis=0)\n",
        "        # Coefficient of variation: normalized measure of consistency\n",
        "        # Lower CV = more consistent (less variation relative to mean)\n",
        "        cv_shap = std_shap / (mean_shap + 1e-9)\n",
        "        \n",
        "        cluster_consistency[cluster_id] = {\n",
        "            'size': int(np.sum(cluster_mask)),\n",
        "            'mean_importance': mean_shap.tolist(),\n",
        "            'std_importance': std_shap.tolist(),\n",
        "            'cv': cv_shap.tolist(),\n",
        "            'mean_cv': float(np.mean(cv_shap))\n",
        "        }\n",
        "\n",
        "print(\"Cluster consistency metrics:\")\n",
        "for cluster_id, metrics in cluster_consistency.items():\n",
        "    print(f\"  Cluster {cluster_id}: size={metrics['size']}, mean_CV={metrics['mean_cv']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Consistency Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "top_features_consistency = importance_df.head(8).index.tolist()\n",
        "top_feature_indices = [feature_names.index(f) for f in top_features_consistency]\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for cluster_id in range(min(4, n_clusters)):\n",
        "    if cluster_id not in cluster_consistency:\n",
        "        continue\n",
        "    \n",
        "    metrics = cluster_consistency[cluster_id]\n",
        "    mean_imp = np.array(metrics['mean_importance'])\n",
        "    std_imp = np.array(metrics['std_importance'])\n",
        "    \n",
        "    top_mean = mean_imp[top_feature_indices]\n",
        "    top_std = std_imp[top_feature_indices]\n",
        "    \n",
        "    x_pos = np.arange(len(top_features_consistency))\n",
        "    axes[cluster_id].bar(x_pos, top_mean, yerr=top_std, alpha=0.7, \n",
        "                        color='#2E86AB', capsize=5)\n",
        "    axes[cluster_id].set_xlabel('Features', fontsize=11, fontweight='bold')\n",
        "    axes[cluster_id].set_ylabel('Mean |SHAP| ± Std', fontsize=11)\n",
        "    axes[cluster_id].set_title(f'Cluster {cluster_id} (n={metrics[\"size\"]}, CV={metrics[\"mean_cv\"]:.3f})', \n",
        "                              fontsize=12, fontweight='bold')\n",
        "    axes[cluster_id].set_xticks(x_pos)\n",
        "    axes[cluster_id].set_xticklabels(top_features_consistency, rotation=45, ha='right', fontsize=9)\n",
        "    axes[cluster_id].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.suptitle('Explanation Consistency Across Clusters', fontsize=15, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, 'explanation_consistency.png'), dpi=300, bbox_inches='tight')\n",
        "plt.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Overall Consistency Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overall mean coefficient of variation: 1.1495\n",
            "Consistency range: [0.8686, 1.5320]\n",
            "Consistency level: Low\n"
          ]
        }
      ],
      "source": [
        "overall_mean_cv = np.mean([m['mean_cv'] for m in cluster_consistency.values()])\n",
        "consistency_scores = [m['mean_cv'] for m in cluster_consistency.values()]\n",
        "\n",
        "print(f\"Overall mean coefficient of variation: {overall_mean_cv:.4f}\")\n",
        "print(f\"Consistency range: [{min(consistency_scores):.4f}, {max(consistency_scores):.4f}]\")\n",
        "\n",
        "# Lower CV = more consistent\n",
        "if overall_mean_cv < 0.5:\n",
        "    consistency_level = \"High\"\n",
        "elif overall_mean_cv < 1.0:\n",
        "    consistency_level = \"Medium\"\n",
        "else:\n",
        "    consistency_level = \"Low\"\n",
        "\n",
        "print(f\"Consistency level: {consistency_level}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results Export and Summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save Model-Specific SHAP Values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_shap_summary = {}\n",
        "\n",
        "for model_name, shap_vals in model_shap_values.items():\n",
        "    importance = np.mean(np.abs(shap_vals), axis=0)\n",
        "    top_features = pd.DataFrame({\n",
        "        'feature': feature_names,\n",
        "        'importance': importance\n",
        "    }).sort_values('importance', ascending=False).head(10)\n",
        "    \n",
        "    model_shap_summary[model_name] = top_features.to_dict('records')\n",
        "\n",
        "with open(os.path.join(OUTPUT_DIR, 'model_specific_shap_summary.json'), 'w') as f:\n",
        "    json.dump(model_shap_summary, f, indent=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate Comprehensive Report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Advanced Explainability Report Summary:\n",
            "  Model SHAP Comparison: 100 samples\n",
            "  Sensitivity Analysis: 50 samples\n",
            "  Decision Boundary: 500 samples, 27.3% variance explained\n",
            "  Consistency Analysis: 200 samples, Low consistency\n",
            "  Top Sensitive Feature: person_income\n",
            "  Most Disagreed Feature: loan_intent_VENTURE\n",
            "\n",
            "All results saved to artifacts/04b_images/\n"
          ]
        }
      ],
      "source": [
        "advanced_report = {\n",
        "    'timestamp': pd.Timestamp.now().isoformat(),\n",
        "    'sample_sizes': {\n",
        "        'model_shap_comparison': int(test_sample_size),\n",
        "        'sensitivity_analysis': int(sensitivity_sample_size),\n",
        "        'decision_boundary': int(db_sample_size),\n",
        "        'consistency_analysis': int(consistency_sample_size)\n",
        "    },\n",
        "    'model_specific_analysis': {\n",
        "        'top_features_by_model': model_shap_summary,\n",
        "        'model_disagreement': {\n",
        "            'top_disagree_features': disagreement_df.head(10)[['feature', 'coefficient_of_variation']].to_dict('records')\n",
        "        }\n",
        "    },\n",
        "    'sensitivity_analysis': {\n",
        "        'top_sensitive_features': sensitivity_df.head(10).to_dict('records'),\n",
        "        'max_sensitivity': float(sensitivity_df.iloc[0]['sensitivity']),\n",
        "        'mean_sensitivity': float(sensitivity_df['sensitivity'].mean())\n",
        "    },\n",
        "    'decision_boundary_analysis': {\n",
        "        'pca_variance_explained': {\n",
        "            'pc1': float(pca.explained_variance_ratio_[0]),\n",
        "            'pc2': float(pca.explained_variance_ratio_[1]),\n",
        "            'total': float(pca.explained_variance_ratio_.sum())\n",
        "        },\n",
        "        'risk_distribution': {\n",
        "            'low_risk': int(np.sum(risk_categories == 0)),\n",
        "            'medium_risk': int(np.sum(risk_categories == 1)),\n",
        "            'high_risk': int(np.sum(risk_categories == 2))\n",
        "        }\n",
        "    },\n",
        "    'explanation_consistency': {\n",
        "        'n_clusters': int(n_clusters),\n",
        "        'overall_mean_cv': float(overall_mean_cv),\n",
        "        'consistency_level': consistency_level,\n",
        "        'cluster_details': cluster_consistency\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(os.path.join(OUTPUT_DIR, 'advanced_explainability_report.json'), 'w') as f:\n",
        "    json.dump(advanced_report, f, indent=2)\n",
        "\n",
        "print(\"Advanced Explainability Report Summary:\")\n",
        "print(f\"  Model SHAP Comparison: {test_sample_size} samples\")\n",
        "print(f\"  Sensitivity Analysis: {sensitivity_sample_size} samples\")\n",
        "print(f\"  Decision Boundary: {db_sample_size} samples, {pca.explained_variance_ratio_.sum():.1%} variance explained\")\n",
        "print(f\"  Consistency Analysis: {consistency_sample_size} samples, {consistency_level} consistency\")\n",
        "print(f\"  Top Sensitive Feature: {sensitivity_df.iloc[0]['feature']}\")\n",
        "print(f\"  Most Disagreed Feature: {disagreement_df.iloc[0]['feature']}\")\n",
        "print(\"\\nAll results saved to artifacts/04b_images/\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "final_last",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
