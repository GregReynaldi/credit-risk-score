{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing and Feature Engineering\n",
        "\n",
        "This notebook prepares the raw credit risk data for machine learning model training.\n",
        "\n",
        "**Purpose**: Transform raw data into model-ready features through cleaning, encoding, and scaling.\n",
        "\n",
        "**Key Steps**:\n",
        "1. **Data Loading**: Load the raw dataset\n",
        "2. **Train/Test Split**: Split data before any preprocessing (prevents data leakage)\n",
        "3. **Outlier Handling**: Cap extreme values to prevent model issues\n",
        "4. **Missing Value Imputation**: Fill missing values using KNN imputation\n",
        "5. **Feature Scaling**: Scale numeric features using RobustScaler\n",
        "6. **Categorical Encoding**: Convert text categories to numbers\n",
        "   - Ordinal encoding for loan_grade (preserves order)\n",
        "   - One-hot encoding for nominal features (no order)\n",
        "   - Binary encoding for yes/no features\n",
        "7. **Save Processed Data**: Save train/test splits and preprocessing components\n",
        "\n",
        "**Critical Principle**: All preprocessing is fit ONLY on training data, then applied to test data. This prevents data leakage and ensures realistic model evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries for data preprocessing\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Scikit-learn components for preprocessing\n",
        "from sklearn.model_selection import train_test_split  # Split data into train/test sets\n",
        "from sklearn.preprocessing import RobustScaler, OneHotEncoder, LabelEncoder  # Feature scaling and encoding\n",
        "from sklearn.impute import KNNImputer  # Missing value imputation using K-Nearest Neighbors\n",
        "\n",
        "import pickle  # For saving preprocessing components (scalers, encoders)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')  # Suppress warnings for cleaner output\n",
        "\n",
        "# Configure visualization style\n",
        "plt.style.use('seaborn-v0_8-whitegrid')  # Clean grid background\n",
        "sns.set_palette(\"husl\")  # Colorful, distinguishable palette"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DIRECTORY SETUP\n",
            "Raw data file: d:\\FINAL PROJECT\\dataset\\credit_risk_dataset.csv\n",
            "Models directory: d:\\FINAL PROJECT\\models\n",
            "Dataset directory: d:\\FINAL PROJECT\\dataset\n",
            "Artifacts directory: d:\\FINAL PROJECT\\artifacts\n"
          ]
        }
      ],
      "source": [
        "# Set up project directory paths\n",
        "# Current directory should be src/, so we go up one level to reach project root\n",
        "ROOT = os.path.abspath(os.getcwd())\n",
        "PROJECT_ROOT = os.path.abspath(os.path.join(ROOT, '..'))\n",
        "\n",
        "# Define key directories\n",
        "DATASET_DIR = os.path.join(PROJECT_ROOT, 'dataset')  # Where raw and processed data are stored\n",
        "MODELS_DIR = os.path.join(PROJECT_ROOT, 'models')  # Where trained models and preprocessors are saved\n",
        "ARTIFACTS_DIR = os.path.join(PROJECT_ROOT, 'artifacts')  # Where analysis results are saved\n",
        "DATA_PATH = os.path.join(DATASET_DIR, 'credit_risk_dataset.csv')  # Path to raw dataset\n",
        "\n",
        "# Create directories if they don't exist\n",
        "# This ensures we can save files without errors\n",
        "os.makedirs(DATASET_DIR, exist_ok=True)\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "os.makedirs(ARTIFACTS_DIR, exist_ok=True)\n",
        "\n",
        "print('DIRECTORY SETUP')\n",
        "print(f\"Raw data file: {DATA_PATH}\")\n",
        "print(f\"Models directory: {MODELS_DIR}\")\n",
        "print(f\"Dataset directory: {DATASET_DIR}\")\n",
        "print(f\"Artifacts directory: {ARTIFACTS_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset shape: (32581, 12)\n",
            "Columns: ['person_age', 'person_income', 'person_home_ownership', 'person_emp_length', 'loan_intent', 'loan_grade', 'loan_amnt', 'loan_int_rate', 'loan_status', 'loan_percent_income', 'cb_person_default_on_file', 'cb_person_cred_hist_length']\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(DATA_PATH)\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 5 rows:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>person_age</th>\n",
              "      <th>person_income</th>\n",
              "      <th>person_home_ownership</th>\n",
              "      <th>person_emp_length</th>\n",
              "      <th>loan_intent</th>\n",
              "      <th>loan_grade</th>\n",
              "      <th>loan_amnt</th>\n",
              "      <th>loan_int_rate</th>\n",
              "      <th>loan_status</th>\n",
              "      <th>loan_percent_income</th>\n",
              "      <th>cb_person_default_on_file</th>\n",
              "      <th>cb_person_cred_hist_length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>22</td>\n",
              "      <td>59000</td>\n",
              "      <td>RENT</td>\n",
              "      <td>123.0</td>\n",
              "      <td>PERSONAL</td>\n",
              "      <td>D</td>\n",
              "      <td>35000</td>\n",
              "      <td>16.02</td>\n",
              "      <td>1</td>\n",
              "      <td>0.59</td>\n",
              "      <td>Y</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>21</td>\n",
              "      <td>9600</td>\n",
              "      <td>OWN</td>\n",
              "      <td>5.0</td>\n",
              "      <td>EDUCATION</td>\n",
              "      <td>B</td>\n",
              "      <td>1000</td>\n",
              "      <td>11.14</td>\n",
              "      <td>0</td>\n",
              "      <td>0.10</td>\n",
              "      <td>N</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>25</td>\n",
              "      <td>9600</td>\n",
              "      <td>MORTGAGE</td>\n",
              "      <td>1.0</td>\n",
              "      <td>MEDICAL</td>\n",
              "      <td>C</td>\n",
              "      <td>5500</td>\n",
              "      <td>12.87</td>\n",
              "      <td>1</td>\n",
              "      <td>0.57</td>\n",
              "      <td>N</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>23</td>\n",
              "      <td>65500</td>\n",
              "      <td>RENT</td>\n",
              "      <td>4.0</td>\n",
              "      <td>MEDICAL</td>\n",
              "      <td>C</td>\n",
              "      <td>35000</td>\n",
              "      <td>15.23</td>\n",
              "      <td>1</td>\n",
              "      <td>0.53</td>\n",
              "      <td>N</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>24</td>\n",
              "      <td>54400</td>\n",
              "      <td>RENT</td>\n",
              "      <td>8.0</td>\n",
              "      <td>MEDICAL</td>\n",
              "      <td>C</td>\n",
              "      <td>35000</td>\n",
              "      <td>14.27</td>\n",
              "      <td>1</td>\n",
              "      <td>0.55</td>\n",
              "      <td>Y</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>21</td>\n",
              "      <td>9900</td>\n",
              "      <td>OWN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>VENTURE</td>\n",
              "      <td>A</td>\n",
              "      <td>2500</td>\n",
              "      <td>7.14</td>\n",
              "      <td>1</td>\n",
              "      <td>0.25</td>\n",
              "      <td>N</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>26</td>\n",
              "      <td>77100</td>\n",
              "      <td>RENT</td>\n",
              "      <td>8.0</td>\n",
              "      <td>EDUCATION</td>\n",
              "      <td>B</td>\n",
              "      <td>35000</td>\n",
              "      <td>12.42</td>\n",
              "      <td>1</td>\n",
              "      <td>0.45</td>\n",
              "      <td>N</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>24</td>\n",
              "      <td>78956</td>\n",
              "      <td>RENT</td>\n",
              "      <td>5.0</td>\n",
              "      <td>MEDICAL</td>\n",
              "      <td>B</td>\n",
              "      <td>35000</td>\n",
              "      <td>11.11</td>\n",
              "      <td>1</td>\n",
              "      <td>0.44</td>\n",
              "      <td>N</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>24</td>\n",
              "      <td>83000</td>\n",
              "      <td>RENT</td>\n",
              "      <td>8.0</td>\n",
              "      <td>PERSONAL</td>\n",
              "      <td>A</td>\n",
              "      <td>35000</td>\n",
              "      <td>8.90</td>\n",
              "      <td>1</td>\n",
              "      <td>0.42</td>\n",
              "      <td>N</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>21</td>\n",
              "      <td>10000</td>\n",
              "      <td>OWN</td>\n",
              "      <td>6.0</td>\n",
              "      <td>VENTURE</td>\n",
              "      <td>D</td>\n",
              "      <td>1600</td>\n",
              "      <td>14.74</td>\n",
              "      <td>1</td>\n",
              "      <td>0.16</td>\n",
              "      <td>N</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   person_age  person_income person_home_ownership  person_emp_length  \\\n",
              "0          22          59000                  RENT              123.0   \n",
              "1          21           9600                   OWN                5.0   \n",
              "2          25           9600              MORTGAGE                1.0   \n",
              "3          23          65500                  RENT                4.0   \n",
              "4          24          54400                  RENT                8.0   \n",
              "5          21           9900                   OWN                2.0   \n",
              "6          26          77100                  RENT                8.0   \n",
              "7          24          78956                  RENT                5.0   \n",
              "8          24          83000                  RENT                8.0   \n",
              "9          21          10000                   OWN                6.0   \n",
              "\n",
              "  loan_intent loan_grade  loan_amnt  loan_int_rate  loan_status  \\\n",
              "0    PERSONAL          D      35000          16.02            1   \n",
              "1   EDUCATION          B       1000          11.14            0   \n",
              "2     MEDICAL          C       5500          12.87            1   \n",
              "3     MEDICAL          C      35000          15.23            1   \n",
              "4     MEDICAL          C      35000          14.27            1   \n",
              "5     VENTURE          A       2500           7.14            1   \n",
              "6   EDUCATION          B      35000          12.42            1   \n",
              "7     MEDICAL          B      35000          11.11            1   \n",
              "8    PERSONAL          A      35000           8.90            1   \n",
              "9     VENTURE          D       1600          14.74            1   \n",
              "\n",
              "   loan_percent_income cb_person_default_on_file  cb_person_cred_hist_length  \n",
              "0                 0.59                         Y                           3  \n",
              "1                 0.10                         N                           2  \n",
              "2                 0.57                         N                           3  \n",
              "3                 0.53                         N                           2  \n",
              "4                 0.55                         Y                           4  \n",
              "5                 0.25                         N                           2  \n",
              "6                 0.45                         N                           3  \n",
              "7                 0.44                         N                           4  \n",
              "8                 0.42                         N                           2  \n",
              "9                 0.16                         N                           3  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"First 5 rows:\")\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 32581 entries, 0 to 32580\n",
            "Data columns (total 12 columns):\n",
            " #   Column                      Non-Null Count  Dtype  \n",
            "---  ------                      --------------  -----  \n",
            " 0   person_age                  32581 non-null  int64  \n",
            " 1   person_income               32581 non-null  int64  \n",
            " 2   person_home_ownership       32581 non-null  object \n",
            " 3   person_emp_length           31686 non-null  float64\n",
            " 4   loan_intent                 32581 non-null  object \n",
            " 5   loan_grade                  32581 non-null  object \n",
            " 6   loan_amnt                   32581 non-null  int64  \n",
            " 7   loan_int_rate               29465 non-null  float64\n",
            " 8   loan_status                 32581 non-null  int64  \n",
            " 9   loan_percent_income         32581 non-null  float64\n",
            " 10  cb_person_default_on_file   32581 non-null  object \n",
            " 11  cb_person_cred_hist_length  32581 non-null  int64  \n",
            "dtypes: float64(3), int64(5), object(4)\n",
            "memory usage: 3.0+ MB\n"
          ]
        }
      ],
      "source": [
        "print(\"Dataset Info:\")\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Target unique values: [1 0]\n",
            "Target dtype: int64\n"
          ]
        }
      ],
      "source": [
        "TARGET_COL = \"loan_status\"\n",
        "print(f\"Target unique values: {df[TARGET_COL].unique()}\")\n",
        "print(f\"Target dtype: {df[TARGET_COL].dtype}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final target distribution:\n",
            "loan_status\n",
            "0    25473\n",
            "1     7108\n",
            "Name: count, dtype: int64\n",
            "Default rate: 21.82%\n"
          ]
        }
      ],
      "source": [
        "print(f\"Final target distribution:\\n{df[TARGET_COL].value_counts()}\")\n",
        "print(f\"Default rate: {(df[TARGET_COL].mean() * 100):.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing Values Summary:\n",
            "                   Missing_Count  Missing_Percentage\n",
            "loan_int_rate               3116            9.563856\n",
            "person_emp_length            895            2.747000\n"
          ]
        }
      ],
      "source": [
        "missing_data = df.isnull().sum()\n",
        "missing_percentage = (missing_data / len(df)) * 100\n",
        "\n",
        "missing_summary = pd.DataFrame({\n",
        "    'Missing_Count': missing_data,\n",
        "    'Missing_Percentage': missing_percentage\n",
        "}).sort_values('Missing_Count', ascending=False)\n",
        "\n",
        "print(\"Missing Values Summary:\")\n",
        "print(missing_summary[missing_summary['Missing_Count'] > 0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Numerical features (7): ['person_age', 'person_income', 'person_emp_length', 'loan_amnt', 'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length']\n",
            "Categorical features (4): ['person_home_ownership', 'loan_intent', 'loan_grade', 'cb_person_default_on_file']\n"
          ]
        }
      ],
      "source": [
        "feature_cols = [col for col in df.columns if col != TARGET_COL]\n",
        "numeric_cols = df[feature_cols].select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_cols = [col for col in feature_cols if col not in numeric_cols]\n",
        "\n",
        "print(f\"Numerical features ({len(numeric_cols)}): {numeric_cols}\")\n",
        "print(f\"Categorical features ({len(categorical_cols)}): {categorical_cols}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAIN/TEST SPLIT COMPLETE\n",
            "Training set: 26,064 samples (80.0%)\n",
            "Test set: 6,517 samples (20.0%)\n",
            "\n",
            "Data split before preprocessing - this prevents data leakage!\n",
            "Stratified split ensures same class balance in train and test sets\n"
          ]
        }
      ],
      "source": [
        "# CRITICAL STEP: Split data into features and target BEFORE any preprocessing\n",
        "# This ensures we don't accidentally use test data information during preprocessing\n",
        "# Separate features (X) from target variable (y)\n",
        "X = df[feature_cols].copy()  # All columns except target\n",
        "y = df[TARGET_COL]  # Target variable (loan_status: 0 = good loan, 1 = bad loan)\n",
        "\n",
        "# Split data into training and test sets\n",
        "# IMPORTANT: This split happens FIRST, before any preprocessing\n",
        "# Why? To prevent data leakage - we must fit preprocessors ONLY on training data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, \n",
        "    test_size=0.2,  # 20% for testing, 80% for training\n",
        "    stratify=y,  # CRITICAL: Maintains same class distribution in both sets\n",
        "                 # Without this, we might get all good loans in train and all bad in test\n",
        "                 # This would break model evaluation and training\n",
        "    random_state=125,  # Set seed for reproducibility (same split every time)\n",
        ")\n",
        "\n",
        "print('TRAIN/TEST SPLIT COMPLETE')\n",
        "print(f\"Training set: {X_train.shape[0]:,} samples ({(X_train.shape[0]/len(X)*100):.1f}%)\")\n",
        "print(f\"Test set: {X_test.shape[0]:,} samples ({(X_test.shape[0]/len(X)*100):.1f}%)\")\n",
        "print(f\"\\nData split before preprocessing - this prevents data leakage!\")\n",
        "print(f\"Stratified split ensures same class balance in train and test sets\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Capped 'person_age': 144 → 100 (removed extreme outliers)\n",
            "Capped 'person_emp_length': 123.0 → 50 (removed extreme outliers)\n",
            "\n",
            "Applied same caps to test data (no data leakage)\n"
          ]
        }
      ],
      "source": [
        "# Handle Outliers: Cap extreme values to prevent them from skewing model training\n",
        "# IMPORTANT: We only cap on training data, then apply the same limits to test data\n",
        "# This prevents data leakage - we don't use test data to determine what's \"normal\"\n",
        "\n",
        "# Cap person_age at 100 years\n",
        "# Reason: Extremely high ages (e.g., 144) are likely data entry errors\n",
        "# We cap at 100 as a reasonable maximum age for loan applicants\n",
        "age_original_max = X_train['person_age'].max()\n",
        "X_train['person_age'] = X_train['person_age'].clip(upper=100)\n",
        "print(f\"Capped 'person_age': {age_original_max} → 100 (removed extreme outliers)\")\n",
        "\n",
        "# Cap person_emp_length at 50 years\n",
        "# Reason: Employment length over 50 years is unrealistic (likely data errors)\n",
        "# We cap at 50 years as a reasonable maximum employment duration\n",
        "emp_original_max = X_train['person_emp_length'].max()\n",
        "X_train['person_emp_length'] = X_train['person_emp_length'].clip(upper=50)\n",
        "print(f\"Capped 'person_emp_length': {emp_original_max} → 50 (removed extreme outliers)\")\n",
        "\n",
        "# Apply the same caps to test data (using limits learned from training data only)\n",
        "X_test['person_age'] = X_test['person_age'].clip(upper=100)\n",
        "X_test['person_emp_length'] = X_test['person_emp_length'].clip(upper=50)\n",
        "print(f\"\\nApplied same caps to test data (no data leakage)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(np.int64(100), np.float64(50.0))"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train['person_age'].max(), X_train['person_emp_length'].max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ordinal features (1): ['loan_grade']\n",
            "Nominal features (2): ['person_home_ownership', 'loan_intent']\n",
            "Binary features (1): ['cb_person_default_on_file']\n"
          ]
        }
      ],
      "source": [
        "# Split categorical columns by encoding type\n",
        "# ORDINAL: loan_grade (A, B, C, D, E, F, G) - has natural order, use LabelEncoder\n",
        "# NOMINAL: person_home_ownership, loan_intent - no order, use OneHotEncoder\n",
        "# BINARY: cb_person_default_on_file (Y/N) - simple mapping\n",
        "ordinal_cols = ['loan_grade']\n",
        "nominal_cols = ['person_home_ownership', 'loan_intent']\n",
        "binary_cols = ['cb_person_default_on_file']\n",
        "\n",
        "print(f\"Ordinal features (1): {ordinal_cols}\")\n",
        "print(f\"Nominal features ({len(nominal_cols)}): {nominal_cols}\")\n",
        "print(f\"Binary features (1): {binary_cols}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "person_age                       0\n",
              "person_income                    0\n",
              "person_home_ownership            0\n",
              "person_emp_length              719\n",
              "loan_intent                      0\n",
              "loan_grade                       0\n",
              "loan_amnt                        0\n",
              "loan_int_rate                 2522\n",
              "loan_percent_income              0\n",
              "cb_person_default_on_file        0\n",
              "cb_person_cred_hist_length       0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MISSING VALUE IMPUTATION\n",
            "Method: KNN Imputer (finds 5 most similar rows)\n",
            "Fitting imputer on training data only...\n",
            "Training data: Missing values imputed\n",
            "Test data: Missing values imputed using training patterns (no data leakage)\n"
          ]
        }
      ],
      "source": [
        "# Missing Value Imputation: Fill in missing values for numeric features\n",
        "# Method: KNN Imputer (K-Nearest Neighbors)\n",
        "# Why KNN instead of mean/median? \n",
        "#   - KNN finds similar rows and uses their values (preserves relationships between features)\n",
        "#   - Mean/median ignores feature relationships (less accurate)\n",
        "#   - Example: If someone has high income but missing employment length, KNN finds similar high-income people\n",
        "#     and uses their employment length (more realistic than just using overall median)\n",
        "\n",
        "# Configure KNN Imputer\n",
        "# n_neighbors=5: Uses 5 most similar rows to impute missing values\n",
        "#   - More neighbors (e.g., 10): Smoother, less noisy, but slower and may underfit\n",
        "#   - Fewer neighbors (e.g., 3): Faster, more precise, but may be noisy and overfit\n",
        "#   - 5 is a good balance for this dataset size\n",
        "numeric_imputer = KNNImputer(n_neighbors=5)\n",
        "\n",
        "# CRITICAL: Fit imputer ONLY on training data, then apply to test data\n",
        "# This prevents data leakage - test data doesn't influence how we handle missing values\n",
        "print('MISSING VALUE IMPUTATION')\n",
        "print('Method: KNN Imputer (finds 5 most similar rows)')\n",
        "print('Fitting imputer on training data only...')\n",
        "\n",
        "# fit_transform(): Learn imputation pattern from training data AND fill missing values\n",
        "X_train_num_imputed = numeric_imputer.fit_transform(X_train[numeric_cols])\n",
        "\n",
        "# transform(): Apply learned pattern to test data (no learning, just application)\n",
        "X_test_num_imputed = numeric_imputer.transform(X_test[numeric_cols])\n",
        "\n",
        "# Convert back to DataFrames to preserve column names and row indices\n",
        "X_train_num_processed = pd.DataFrame(\n",
        "    X_train_num_imputed,\n",
        "    columns=numeric_cols,\n",
        "    index=X_train.index\n",
        ")\n",
        "\n",
        "X_test_num_processed = pd.DataFrame(\n",
        "    X_test_num_imputed,\n",
        "    columns=numeric_cols,\n",
        "    index=X_test.index\n",
        ")\n",
        "\n",
        "print(f\"Training data: Missing values imputed\")\n",
        "print(f\"Test data: Missing values imputed using training patterns (no data leakage)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FEATURE SCALING\n",
            "Method: RobustScaler (uses median and IQR, robust to outliers)\n",
            "Fitting scaler on training data only...\n",
            "Training data: Features scaled\n",
            "Test data: Features scaled using training parameters (no data leakage)\n",
            "Scaler saved to: d:\\FINAL PROJECT\\models\\RobutScaler.pkl (for use in prediction pipeline)\n"
          ]
        }
      ],
      "source": [
        "# Feature Scaling: Scale numeric features to similar ranges\n",
        "# Why scale? Different features have different units and ranges (e.g., income in thousands, age in years)\n",
        "# Scaling ensures all features contribute equally to model training\n",
        "\n",
        "# Method: RobustScaler (instead of StandardScaler)\n",
        "# Why RobustScaler?\n",
        "#   - StandardScaler uses mean and standard deviation (sensitive to outliers)\n",
        "#   - RobustScaler uses median and IQR (Interquartile Range) - robust to outliers\n",
        "#   - Our dataset has outliers (from EDA), so RobustScaler is more appropriate\n",
        "#   - Median: Middle value (not affected by extreme values)\n",
        "#   - IQR: Spread between 25th and 75th percentile (measures variability without outliers)\n",
        "\n",
        "robust_scaler = RobustScaler()\n",
        "\n",
        "# CRITICAL: Fit scaler ONLY on training data, then apply to test data\n",
        "# This prevents data leakage - test data doesn't influence scaling parameters\n",
        "print('FEATURE SCALING')\n",
        "print('Method: RobustScaler (uses median and IQR, robust to outliers)')\n",
        "print('Fitting scaler on training data only...')\n",
        "\n",
        "# fit_transform(): Learn scaling parameters (median, IQR) from training data AND scale\n",
        "X_train_num_scaled = robust_scaler.fit_transform(X_train_num_processed)\n",
        "\n",
        "# transform(): Apply learned scaling to test data (no learning, just application)\n",
        "X_test_num_scaled = robust_scaler.transform(X_test_num_processed)\n",
        "\n",
        "# Convert back to DataFrames to preserve column names and row indices\n",
        "X_train_num_scaled = pd.DataFrame(\n",
        "    X_train_num_scaled, \n",
        "    columns=numeric_cols, \n",
        "    index=X_train.index\n",
        ")\n",
        "X_test_num_scaled = pd.DataFrame(\n",
        "    X_test_num_scaled, \n",
        "    columns=numeric_cols, \n",
        "    index=X_test.index\n",
        ")\n",
        "\n",
        "# Save the scaler for later use in prediction pipeline\n",
        "# When we make predictions on new data, we need to apply the same scaling\n",
        "scaler_path = os.path.join(MODELS_DIR, 'RobutScaler.pkl')\n",
        "with open(scaler_path, 'wb') as f:\n",
        "    pickle.dump(robust_scaler, f)\n",
        "print(f\"Training data: Features scaled\")\n",
        "print(f\"Test data: Features scaled using training parameters (no data leakage)\")\n",
        "print(f\"Scaler saved to: {scaler_path} (for use in prediction pipeline)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CATEGORICAL FEATURE ENCODING\n",
            "\n",
            "1. Ordinal Encoding (loan_grade):\n",
            "   Method: LabelEncoder (preserves order: A < B < C < D < E < F < G)\n",
            "Encoded: 1 feature(s)\n",
            "\n",
            "2. Nominal Encoding (person_home_ownership, loan_intent):\n",
            "   Method: OneHotEncoder (creates binary columns, no order assumption)\n",
            "Encoded: 2 feature(s) → 8 binary columns\n",
            "\n",
            "3. Binary Encoding (cb_person_default_on_file):\n",
            "   Method: Direct mapping (Y → 1, N → 0)\n",
            "Encoded: 1 feature(s)\n",
            "ENCODING SUMMARY\n",
            "LabelEncoder saved to: d:\\FINAL PROJECT\\models\\LabelEncoder.pkl\n",
            "OneHotEncoder saved to: d:\\FINAL PROJECT\\models\\OneHotEncoder.pkl\n",
            "Total encoded categorical features: 10 columns\n",
            "All encoders fitted on training data only (no data leakage)\n"
          ]
        }
      ],
      "source": [
        "# Categorical Feature Encoding: Convert text categories to numbers\n",
        "# Different encoding strategies for different types of categorical features\n",
        "# Strategy chosen based on whether categories have natural order or not\n",
        "\n",
        "print('CATEGORICAL FEATURE ENCODING')\n",
        "\n",
        "# ============================================================================\n",
        "# ORDINAL ENCODING: loan_grade (has natural order: A < B < C < D < E < F < G)\n",
        "# ============================================================================\n",
        "# Method: LabelEncoder\n",
        "# Why? Preserves the natural order (A=0, B=1, C=2, etc.)\n",
        "# Models can learn that higher numbers = worse grade = higher risk\n",
        "print('\\n1. Ordinal Encoding (loan_grade):')\n",
        "print('   Method: LabelEncoder (preserves order: A < B < C < D < E < F < G)')\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "# fit_transform(): Learn category order from training data AND encode\n",
        "X_train_ordinal = label_encoder.fit_transform(X_train[ordinal_cols].values.ravel())\n",
        "# transform(): Apply learned mapping to test data\n",
        "X_test_ordinal = label_encoder.transform(X_test[ordinal_cols].values.ravel())\n",
        "X_train_ordinal_df = pd.DataFrame(X_train_ordinal, columns=ordinal_cols, index=X_train.index)\n",
        "X_test_ordinal_df = pd.DataFrame(X_test_ordinal, columns=ordinal_cols, index=X_test.index)\n",
        "print(f\"Encoded: {len(ordinal_cols)} feature(s)\")\n",
        "\n",
        "# ============================================================================\n",
        "# NOMINAL ENCODING: person_home_ownership, loan_intent (no natural order)\n",
        "# ============================================================================\n",
        "# Method: OneHotEncoder\n",
        "# Why? No order assumption - each category gets its own binary column\n",
        "# Example: RENT → [1,0,0], OWN → [0,1,0], MORTGAGE → [0,0,1]\n",
        "print('\\n2. Nominal Encoding (person_home_ownership, loan_intent):')\n",
        "print('   Method: OneHotEncoder (creates binary columns, no order assumption)')\n",
        "\n",
        "one_hot_encoder = OneHotEncoder(\n",
        "    drop='first',  # Remove first category to avoid multicollinearity (redundant info)\n",
        "    sparse_output=False,  # Return regular arrays (not sparse matrices)\n",
        "    handle_unknown='ignore'  # If test set has unseen category, set all columns to 0\n",
        ")\n",
        "# fit_transform(): Learn all categories from training data AND create binary columns\n",
        "X_train_nominal = one_hot_encoder.fit_transform(X_train[nominal_cols])\n",
        "# transform(): Apply learned categories to test data\n",
        "X_test_nominal = one_hot_encoder.transform(X_test[nominal_cols])\n",
        "nominal_feature_names = one_hot_encoder.get_feature_names_out(nominal_cols)\n",
        "X_train_nominal_df = pd.DataFrame(X_train_nominal, columns=nominal_feature_names, index=X_train.index)\n",
        "X_test_nominal_df = pd.DataFrame(X_test_nominal, columns=nominal_feature_names, index=X_test.index)\n",
        "print(f\"Encoded: {len(nominal_cols)} feature(s) → {len(nominal_feature_names)} binary columns\")\n",
        "\n",
        "# ============================================================================\n",
        "# BINARY ENCODING: cb_person_default_on_file (simple yes/no)\n",
        "# ============================================================================\n",
        "# Method: Simple replacement (no encoder needed)\n",
        "# Why? Just two values, so direct mapping is simplest: \"Y\" → 1, \"N\" → 0\n",
        "print('\\n3. Binary Encoding (cb_person_default_on_file):')\n",
        "print('   Method: Direct mapping (Y → 1, N → 0)')\n",
        "\n",
        "X_train_binary = X_train[binary_cols].replace({'Y': 1, 'N': 0}).values\n",
        "X_test_binary = X_test[binary_cols].replace({'Y': 1, 'N': 0}).values\n",
        "X_train_binary_df = pd.DataFrame(X_train_binary, columns=binary_cols, index=X_train.index)\n",
        "X_test_binary_df = pd.DataFrame(X_test_binary, columns=binary_cols, index=X_test.index)\n",
        "print(f\"Encoded: {len(binary_cols)} feature(s)\")\n",
        "\n",
        "# ============================================================================\n",
        "# COMBINE ALL ENCODED CATEGORICAL FEATURES\n",
        "# ============================================================================\n",
        "# Concatenate all encoded features horizontally (side by side)\n",
        "X_train_cat_processed = pd.concat([X_train_ordinal_df, X_train_nominal_df, X_train_binary_df], axis=1)\n",
        "X_test_cat_processed = pd.concat([X_test_ordinal_df, X_test_nominal_df, X_test_binary_df], axis=1)\n",
        "\n",
        "# ============================================================================\n",
        "# SAVE ENCODERS FOR INFERENCE PIPELINE\n",
        "# ============================================================================\n",
        "# These encoders must be saved and reused when making predictions on new data\n",
        "# They ensure new data is encoded the same way as training data\n",
        "label_encoder_path = os.path.join(MODELS_DIR, 'LabelEncoder.pkl')\n",
        "one_hot_encoder_path = os.path.join(MODELS_DIR, 'OneHotEncoder.pkl')\n",
        "with open(label_encoder_path, 'wb') as f:\n",
        "    pickle.dump(label_encoder, f)\n",
        "with open(one_hot_encoder_path, 'wb') as f:\n",
        "    pickle.dump(one_hot_encoder, f)\n",
        "\n",
        "print('ENCODING SUMMARY')\n",
        "print(f\"LabelEncoder saved to: {label_encoder_path}\")\n",
        "print(f\"OneHotEncoder saved to: {one_hot_encoder_path}\")\n",
        "print(f\"Total encoded categorical features: {X_train_cat_processed.shape[1]} columns\")\n",
        "print(f\"All encoders fitted on training data only (no data leakage)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Put numeric and categorical features back together\n",
        "X_train_processed = pd.concat([X_train_num_scaled, X_train_cat_processed], axis=1)\n",
        "X_test_processed = pd.concat([X_test_num_scaled, X_test_cat_processed], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SAVING PROCESSED DATASETS\n",
            "Training features saved: d:\\FINAL PROJECT\\dataset\\X_train.pkl (26,064 samples, 17 features)\n",
            "Test features saved: d:\\FINAL PROJECT\\dataset\\X_test.pkl (6,517 samples, 17 features)\n",
            "Training targets saved: d:\\FINAL PROJECT\\dataset\\y_train.pkl (26,064 samples)\n",
            "Test targets saved: d:\\FINAL PROJECT\\dataset\\y_test.pkl (6,517 samples)\n",
            "PREPROCESSING COMPLETE!\n",
            "All datasets are ready for model training\n",
            "Preprocessing components saved (scalers, encoders)\n",
            "No data leakage - all preprocessing fitted on training data only\n",
            "\n",
            "Next step: Train machine learning models using the processed datasets.\n"
          ]
        }
      ],
      "source": [
        "# Save Processed Datasets\n",
        "# These will be used for model training and evaluation\n",
        "# All preprocessing is complete: missing values filled, features scaled, categories encoded\n",
        "\n",
        "print('SAVING PROCESSED DATASETS')\n",
        "\n",
        "# Define file paths\n",
        "X_train_path = os.path.join(DATASET_DIR, 'X_train.pkl')\n",
        "X_test_path = os.path.join(DATASET_DIR, 'X_test.pkl')\n",
        "y_train_path = os.path.join(DATASET_DIR, 'y_train.pkl')\n",
        "y_test_path = os.path.join(DATASET_DIR, 'y_test.pkl')\n",
        "\n",
        "# Save processed feature sets\n",
        "with open(X_train_path, 'wb') as f:\n",
        "    pickle.dump(X_train_processed, f)\n",
        "print(f\"Training features saved: {X_train_path} ({X_train_processed.shape[0]:,} samples, {X_train_processed.shape[1]} features)\")\n",
        "\n",
        "with open(X_test_path, 'wb') as f:\n",
        "    pickle.dump(X_test_processed, f)\n",
        "print(f\"Test features saved: {X_test_path} ({X_test_processed.shape[0]:,} samples, {X_test_processed.shape[1]} features)\")\n",
        "\n",
        "# Save target variables\n",
        "with open(y_train_path, 'wb') as f:\n",
        "    pickle.dump(y_train, f)\n",
        "print(f\"Training targets saved: {y_train_path} ({len(y_train):,} samples)\")\n",
        "\n",
        "with open(y_test_path, 'wb') as f:\n",
        "    pickle.dump(y_test, f)\n",
        "print(f\"Test targets saved: {y_test_path} ({len(y_test):,} samples)\")\n",
        "\n",
        "print('PREPROCESSING COMPLETE!')\n",
        "print('All datasets are ready for model training')\n",
        "print('Preprocessing components saved (scalers, encoders)')\n",
        "print('No data leakage - all preprocessing fitted on training data only')\n",
        "print('\\nNext step: Train machine learning models using the processed datasets.')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "final_last",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
